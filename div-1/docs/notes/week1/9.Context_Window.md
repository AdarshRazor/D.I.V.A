# Day 3: Context Window - The Working Memory

> **The 80/20 Takeaway:** The "Context Window" is the amount of text the model can "see" at once. It includes your prompt, the conversation history, and the answer. If you exceed it, the model "forgets" the beginning. **Rule:** Keep it clean. Garbage in, Garbage out.

---

## 1. The Core Mental Model: "The RAM Limit"

Think of the Context Window like the RAM in your computer, not the Hard Drive.
*   **Hard Drive (Training Data):** The model knows "Paris is in France" forever.
*   **RAM (Context Window):** The model only knows "My name is Adarsh" *while that sentence is in the window*.

### How it's measured
*   **Tokens:** 1,000 tokens $\approx$ 750 words.
*   **Standard Sizes:**
    *   GPT-4: 128k tokens ($\approx$ 300 pages).
    *   Claude 3: 200k+ tokens.
    *   Llama-3 (Local): 8k - 128k tokens (Depends on your RAM).

---

## 2. The "Lost in the Middle" Phenomenon

Just because a model *can* read 100 pages doesn't mean it pays attention to all of them.
*   **Primacy Effect:** Models pay most attention to the **Beginning** of the prompt.
*   **Recency Effect:** Models pay high attention to the **End** of the prompt.
*   **The Middle:** Information buried in the middle often gets ignored or hallucinated.

**Implication for D.I.V.A:**
*   Don't dump 50 emails into the prompt and ask for a summary.
*   Put the *most critical* instruction at the **End** of the prompt.

---

## 3. Context Engineering Strategies (80/20)

How to maximize your window:

1.  **Filtering (The 80/20 Rule):**
    *   Don't send the full HTML of a website. Extract only the text.
    *   Don't send the full email thread if only the last 3 replies matter.
2.  **Summarization:**
    *   If a conversation gets too long, ask the LLM to "Summarize our chat so far" and replace the history with that summary.
3.  **RAG (Retrieval):**
    *   Instead of putting *all* your notes in the window, find the 3 most relevant notes (Vectors!) and put only those in.

---

## 4. Summary for D.I.V.A

*   **Cost:** You pay per token. Filling the window is expensive.
*   **Latency:** More tokens = Slower generation.
*   **Design:** Build your "Memory" system to fetch only the *exact* context needed for the current question.
