# Day 1: Next Token Prediction - The "Dice Roll" of AI

> **The 80/20 Takeaway:** LLMs don't "write" text; they **roll dice** to pick the next word. You control the dice with **Temperature**. Low temperature = loaded dice (predictable). High temperature = fair dice (creative/random).

---

## 1. The Core Mental Model: "The Probability Tree"

When you ask "What is the capital of France?", the LLM calculates the probability for *every word in its vocabulary* to be the next one.

**The Internal List (Logits):**
1.  `Paris` (99.0%)
2.  `The` (0.5%)
3.  `Lyon` (0.1%)
4.  `Banana` (0.00001%)

It then **samples** (picks) one based on these odds.

### Why it matters for D.I.V.A:
*   **Non-Determinism:** Even with 99% odds, sometimes it picks `The` or `Lyon`. This is why you can get different answers to the same question.
*   **Creativity vs. Accuracy:** You can force it to always pick #1 (Accuracy) or let it gamble on #2 or #3 (Creativity).

---

## 2. Controlling the Dice: Temperature & Sampling

You have two main knobs to turn when building your assistant.

### A. Temperature (The "Chaos" Slider)
*   **0.0 (The Robot):** "Greedy Decoding." It *always* picks the #1 most likely token.
    *   *Use for:* Coding, JSON extraction, factual Q&A.
    *   *Effect:* Rigid, repetitive, but reliable.
*   **0.7 - 1.0 (The Human):** Standard setting. It mostly picks #1 but sometimes picks #2 or #3 if they are close.
    *   *Use for:* Chat, email writing, summarizing.
    *   *Effect:* Natural, fluent, slightly variable.
*   **> 1.0 (The Poet/Madman):** It flattens the odds, making `Banana` almost as likely as `Paris`.
    *   *Use for:* Brainstorming, creative writing.
    *   *Effect:* Wild, hallucination-prone, "creative."

### B. Top-P (Nucleus Sampling)
A smarter alternative to Temperature.
*   **Concept:** "Only look at the top X% of likely words."
*   **Top-P = 0.9:** The model looks at the top words that add up to 90% probability and *cuts off the rest*.
*   **Why use it?** It prevents the model from ever picking `Banana` (the crazy low-probability words) while still allowing variety among the good words (`Paris`, `Lyon`).

---

## 3. Summary for D.I.V.A

1.  **For your "Orchestrator" (Routing/Tools):**
    *   **Set Temperature = 0.** You want it to call the *exact* tool name, not a creative variation.
2.  **For your "Chat Interface":**
    *   **Set Temperature = 0.7.** You want it to sound human and engaging.
3.  **For "Memory/Summarization":**
    *   **Set Temperature = 0.** You want a factual summary, not a creative reinterpretation of your day.

> **Rule of Thumb:** If you are building a system (JSON, Code, Tools), use **0**. If you are talking to a human, use **0.7**.
