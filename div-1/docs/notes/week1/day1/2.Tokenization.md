# Day 1: Tokenization - The Bridge Between Text and Math

> **The 80/20 Takeaway:** LLMs do not see words or letters; they see **Tokens** (integers). This "translation layer" is the root cause of many LLM quirks: why they can't spell "strawberry," why they are bad at math, and how you get billed.

---

## 1. The Core Mental Model: "The Lego Brick"

Imagine trying to build a sentence out of pre-made Lego bricks.
*   **Common words** are single bricks: `[apple]`, `[run]`.
*   **Complex words** are stacks of bricks: `[un]`, `[friend]`, `[li]`, `[ness]`.
*   **Rare words** are piles of single-letter bricks: `[x]`, `[y]`, `[z]`.

**Tokenization** is the process of chopping your text into these bricks *before* the LLM ever sees it.

### Why it matters for D.I.V.A:
1.  **Cost:** You pay per brick (token), not per word.
2.  **Memory:** The "Context Window" is a limit on bricks, not words.
3.  **Bugs:** If a concept is split across weird bricks, the model struggles to understand it.

---

## 2. The "Strawberry" & Math Problem

Why can't GPT-4 count the 'r's in "strawberry"?

*   **The Human View:** S-t-r-a-w-b-e-r-r-y (10 letters, 3 'r's).
*   **The LLM View:** `[straw]` (token 856) + `[berry]` (token 412).
    *   The model sees the integer `856` and `412`. It *never sees* the letters 'r-r' inside `[berry]`.
    *   It has to *memorize* that "token 412 has two r's" rather than *counting* them.

**Why Math Fails:**
*   Input: "9.11 vs 9.9"
*   Tokens: `[9]`, `[.]`, `[11]` vs `[9]`, `[.]`, `[9]`.
*   The model treats `[11]` as a single concept (eleven), not two ones. It might think "11 is bigger than 9, so 9.11 > 9.9", failing to treat it as a decimal.

---

## 3. Key Concepts for Developers

### A. Byte Pair Encoding (BPE)
This is the standard algorithm used by almost all modern LLMs (GPT, Llama).
*   **How it works:** It starts with single characters and iteratively merges the most common pairs.
    *   `t`, `h`, `e` $\rightarrow$ `the` (common, becomes 1 token).
    *   `Z`, `y`, `x` $\rightarrow$ stays as 3 tokens (rare).
*   **Efficiency:** English is very efficient (~0.75 words/token). Code is efficient.
*   **Inefficiency:** Non-Latin scripts, random strings, and URLs can be very inefficient (many tokens per word).

### B. The "Tiktoken" Rule
Always check your token counts.
*   **Rule of Thumb:** 1,000 tokens $\approx$ 750 words (English).
*   **Tool:** Use OpenAI's [Tiktoken](https://github.com/openai/tiktoken) library (or similar for other models) to count tokens *before* sending a request.
*   **D.I.V.A Tip:** When you build your "Memory" system, you will need to chunk text. **Chunk by tokens, not characters**, to ensure you fit exactly into the context window.

---

## 4. Summary for D.I.V.A

1.  **Billing:** Monitor your token usage. A "chatty" system that sends long prompts back and forth drains money/compute fast.
2.  **Context Management:** You have a fixed budget (e.g., 8k, 128k tokens). Don't fill it with junk.
3.  **Debugging:** If the model acts weird on a specific word or number, check how it's tokenized. It might be getting chopped into garbage.
