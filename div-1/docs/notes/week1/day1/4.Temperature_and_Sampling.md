# Day 1: Temperature & Sampling - The Control Knobs

> **The 80/20 Takeaway:** "Temperature" is just one way to control randomness. Modern LLMs use a mix of **Temperature**, **Top-P**, and **Min-P** to balance creativity and coherence. Mastering these knobs is the difference between a "smart" assistant and a "hallucinating" one.

---

## 1. The Cheat Sheet: Which Parameters to Use?

| Goal | Temp | Top-P | Min-P | Use Case |
| :--- | :--- | :--- | :--- | :--- |
| **Code / Math / JSON** | `0.0` | `1.0` | `0.0` | You want the *single best* answer. No creativity allowed. |
| **Chat / Q&A** | `0.7` | `0.9` | `0.05` | Natural, human-like flow. Cuts off "crazy" low-prob words. |
| **Creative Writing** | `1.1` | `0.95` | `0.1` | High creativity, but "Min-P" keeps it grounded. |
| **Brainstorming** | `1.5` | `0.98` | `0.0` | Wild ideas. Expect some nonsense. |

---

## 2. The Parameters Explained

### A. Temperature (The "Flattener")
*   **What it does:** It flattens or sharpens the probability curve.
*   **High Temp (>1):** Makes rare words more likely.
*   **Low Temp (<1):** Makes common words *dominatingly* likely.
*   **Mental Model:** Think of it as "melting" the probability bar chart.

### B. Top-K (The "Hard Cutoff")
*   **What it does:** "Only look at the top 50 words."
*   **Problem:** If the top 50 words are all bad (or all good), it doesn't adapt. It's a blunt instrument.
*   **Status:** *Obsolete* for most high-quality chat. Use Top-P instead.

### C. Top-P (Nucleus Sampling - The "Smart Cutoff")
*   **What it does:** "Keep picking words until their probabilities add up to 90%."
*   **Why it's better:**
    *   If the model is unsure (flat distribution), it keeps *many* words.
    *   If the model is sure (sharp distribution), it keeps *few* words.
*   **Recommendation:** Always use Top-P (e.g., 0.9) alongside Temperature.

### D. Min-P (The "New Standard")
*   **What it does:** "Ignore any word that is less than X% as likely as the *best* word."
*   **Example:** If the best word is 50% likely, and Min-P is 0.1, it ignores anything below 5% (50% * 0.1).
*   **Why it's great:** It dynamically cleans up "garbage" tokens without killing diversity. It's safer than high temperature alone.

---

## 3. Advanced Strategy: "Mirostat"
*   **Concept:** Instead of setting a fixed temperature, you set a **Target Perplexity** (i.e., "I want the text to be *this* surprising").
*   **Mechanism:** The model adjusts its own temperature *per token* to keep the "surprise level" constant.
*   **Use Case:** Long-form story writing where you don't want the model to get "bored" (repetitive) or "crazy" (incoherent).

---

## 4. Summary for D.I.V.A

1.  **Default Config:** Start with `Temp=0.7`, `Top-P=0.9`.
2.  **For Tools:** Force `Temp=0`.
3.  **For "Personality":** If you want a "witty" Jarvis, try `Temp=1.1` but add `Min-P=0.05` to stop it from speaking gibberish.
