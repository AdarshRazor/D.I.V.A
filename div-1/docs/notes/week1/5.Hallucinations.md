# Day 1: Hallucinations - When the AI Dreams

> **The 80/20 Takeaway:** Hallucinations are not "bugs"; they are a feature of the probabilistic engine. The model is *always* guessing. When it guesses wrong but sounds confident, we call it a hallucination. You cannot "fix" this, you can only **mitigate** it with Grounding (RAG).

---

## 1. The Core Mental Model: "The Confident Bullshitter"

Think of an LLM as a brilliant improv actor who has read the entire internet but has no access to a fact-checker.
*   **The Goal:** The actor's job is to *never stop talking* and *always sound plausible*.
*   **The Failure Mode:** If you ask "Who won the 2028 Olympics?", the actor will invent a winner because saying "I don't know" is less statistically likely than generating a name.

### Types of Hallucinations
1.  **Factual Hallucination:** "The Eiffel Tower is in Berlin." (Wrong fact).
2.  **Faithfulness Hallucination:** You give it a document about "Apples" and ask for a summary, but it adds info about "Oranges" that wasn't in the text. (Unfaithful to source).

---

## 2. Why It Happens (The "Blurry JPEG" Again)

Remember **Compression**?
*   The model didn't store the *exact* sentence "The Eiffel Tower is in Paris."
*   It stored the *probability* that "Tower" + "Paris" appear together.
*   If it has seen "Berlin" and "Tower" together in other contexts (e.g., TV Tower), there is a non-zero chance it mixes them up.

---

## 3. How to Spot & Fix Them

### A. The "Sniff Test" (Spotting)
*   **Citations:** If it cites "Smith et al., 2022", check it. LLMs love to invent fake papers.
*   **Numbers:** If it gives a specific number (e.g., "$12,345 revenue"), it's likely made up unless you provided that number in the context.
*   **URLs:** 90% of generated URLs are dead links.

### B. Mitigation Strategies (Fixing)
You cannot train hallucination out of a model. You must **architect** around it.

1.  **Grounding (RAG):**
    *   *Don't ask:* "What is my company's vacation policy?" (Relies on internal weights -> Hallucination).
    *   *Do ask:* "Here is the policy document. Based *only* on this text, what is the vacation policy?" (Relies on Context -> Accurate).

2.  **"I Don't Know" Prompting:**
    *   Add this to your system prompt: *"If the answer is not in the provided context, state 'I don't know'. Do not make up an answer."*

3.  **Low Temperature:**
    *   As learned in the previous section, use `Temperature=0` for factual tasks to reduce the "creative" guessing.

---

## 4. Summary for D.I.V.A

For your assistant to be trusted:
1.  **Never** let it answer from its own "brain" about your personal life.
2.  **Always** fetch the relevant data (email, calendar) first, put it in the prompt, and ask it to *summarize* that data.
3.  **Verify** critical actions (e.g., "Sending email to Bob"). Don't let it auto-send without you checking the address.
